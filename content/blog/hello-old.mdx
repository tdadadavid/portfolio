---
title: Mixture of EXperts (MoE) architecture
summary: Deep dive into MoEs and building CNNs of our own.
slug: hello-old
tags: ['Backend', 'ML']
publishedOn: '2025-04-8T:15:00:00Z'
year: '2024'
---

# Test Old

Hello again
